---
layout: post
title: "Support Vector Machine(SVM)"
subtitle: "서포트벡터머신"
date: 2024-11-17 16:00:00 +0900
categories: [dev]
---

# **수정중**

#### 들어갈 내용
- threshold
- margin
- 두 점사이의 거리
- 벡터 내적의 개념
- 직교
- 법선벡터
- 정사영
- 비선형계획문제의 해법(라그랑지, 쿤터커)
- 다항 커널 사용 (a*b+r)^d꼴에서
  if r=1/2일때
  (a,a^2,1/2)*(b,b^2,1/2) 꼴의 dot product 형태로 나타낼 수 있음
- Radial Kernel 사용

---
# Support Vector Machine

두 데이터를 나누는 threshold(2차원에선 직선, 3차원에선 초평면(Hyperplane)  
마진(margin) : threshold와 가장 가까운 관측지들과의 거리.

---
![svm](/assets/images/SVM_margin.png)  
Margin이 넓을 수록 이후에 입력될 데이터를 정확히 분류할 확률이 올라감.  
즉, **SVM의 목적**은 이 **Margin을 최대화**하는 것.  

---

## 내적의 관점

### 1. 성분분해
![성분분해](/assets/images/Dot_Product.png)
   
### 2. 각도, 길이를 이용(**SVM**에서 사용)  
![각도](/assets/images/Angle_Product.png)

### Decision Rule
decision boundary(우리가 구할려고 하는 직선 or hyperplane)  
#### w: 직선(decision boundary)에 직교하는 벡터(법선벡터)  
#### x: 직선위의 점(벡터)들  

$$
w \cdot u = |w| |u| \cos\theta = d
$$

$$
w \cdot x = d
$$

![decision rule](/assets/images/Decision_Rule.png)

$$
w \cdot x = |w| |x| \cos\theta = d
$$

$$
w \cdot u \geq d \quad \text{? Then } + \, \text{(선 위쪽)}
$$

$$
w \cdot u \leq d \quad \text{? Then } - \, \text{(선 아래쪽)}
$$

$$
w \cdot u + b \geq 0 \quad \text{? Then } + \, \text{(선 위쪽)}
$$

$$
w \cdot u + b \leq 0 \quad \text{? Then } - \, \text{(선 아래쪽)}
$$

#### dicision boundary를 구하는 과정을 따라가다 보면 최종적으로 w만 가지고 마진을 최대화하는 목적함수를 도출할 수 있음.  

$$
\text{margin} = \frac{2}{\|w\|^2}
$$

---

## 마진 극대화:  

$$
\max \frac{2}{\|w\|^2} = \min \frac{1}{2} \|w\|^2
$$

### s.t.

$$
y_i (w \cdot x + b) \geq 1
$$  

$$
y_i =
\begin{cases} 
1 & \text{for } + \\
-1 & \text{for } -
\end{cases}
$$

SVM은 마진(위 목적함수)을 최대화하는 문제  
-> Maximum 문제로 목적함수와 제약식꼴로 나타남.  
제약 조건이 있는 경우엔 제약 조건을 목적함수에 대입.  
why? 제약 조건을 자동으로 만족시키는 해를 찾도록 문제를 변형할 수 있음.  

### i) 제약 조건이 등식인 경우(라그랑지 방법).  
   - 라그랑지 승수(Lagrange Multiplier)를 도입.   
   - 다른 변수들이 일정할 때 제약조건식의 우변상수인 b가 변화함으로써 목적함수 f가 변화되는 양을 의미.  

#### 원 함수:

$$
Maximize(Minimize) f(x)
$$

s.t.

$$
g(x) = b
$$

#### 라그랑지 함수:

$$
\mathcal{L}(x, \lambda) = f(x) + \lambda \big(b - g(x)\big)
$$

#### 라그랑지 방정식에서 편미분:

$$
\frac{\partial \mathcal{L}}{\partial x} = \frac{\partial f(x)}{\partial x} - \lambda \frac{\partial g(x)}{\partial x} = 0
$$

$$
\lambda = \frac{\partial f(x)}{\partial g(x)}
$$

### ii) 제약 조건이 부등식인 경우(쿤-터커 조건법).
   - 등식으로 전환 후 라그랑지 승수 도입.
   - 새로운 인위변수 $$\ s_i^2 \$$ 를 제약 조건 좌변에 더해줌.
     
$$
s_i^2 = b_i - g_i(x)
$$

$$
\mathcal{L}(x_j, s_i, \lambda_i) = f(x_j) - \sum_{i=1}^m \lambda_i \big(g_i(x_j) - b_i + s_i^2\big)
$$

$$
\frac{\partial \mathcal{L}}{\partial x_j} =
\frac{\partial f}{\partial x_j} -
\sum_{i=1}^m \lambda_i \frac{\partial g_i}{\partial x_j} = 0
$$

$$
\frac{\partial \mathcal{L}}{\partial \lambda_i} = -(g_i(x_j) - b_i + s_i^2) = 0
$$

$$
\frac{\partial \mathcal{L}}{\partial s_i} = -2\lambda_i s_i = 0
$$

$$
\lambda_i \text{ 또는 } s_i \text{ 중 적어도 하나는 } 0
$$

##### 이러한 과정을 통해 **마진을 극대화하는 선 또는 초평면**의 방정식을 구하는 것.  

### soft margin
데이터의 분류 오류를 허용하면서도 Hyperplane을 최대 마진으로 설정.  

#### 마진 극대화: 

$$
\min \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n \xi_i
$$

#### 이때 w는 $$L_2$$ norm.  

s.t.  
$$
y_i (w \cdot x_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0 \quad \forall i
$$

#### 설명
- $$\frac{1}{2} \|w\|^2$$: 마진의 크기를 최소화하여 최대 마진을 찾음.
- $$\sum_{i=1}^n \xi_i$$: 분류 오류의 합을 최소화.
- $$C$$: 마진과 분류 오류 간의 트레이드오프를 조절하는 하이퍼파라미터.

but, 직선 또는 초평면으로 나눠지지 않는 데이터가 있음.  

#### Kernel Function 함수를 이용.  
DL에서 Activation Function 과 같은 개념  
여러 커널함수 중에 다항, 가우시안을 많이 씀.  
차원을 이동하지만 실제 데이터를 이동하는게 아니라 트릭을 써서 원래 데이터로 표현할 수 있다.  

## Polynomial Kernel

$$
(a*b+r)^d
$$

내적 형태(Dot Product)로 나타남.  

$$
(a, a^2,\frac{1}{2})*(b,b^2,\frac{1}{2})  
$$
