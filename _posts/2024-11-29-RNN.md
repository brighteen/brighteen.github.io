---
layout: post
title: "Recurrent Neural Network, RNN(수정중)"
subtitle: "transformer를 향해 2"
date: 2024-11-29 18:30:00 +0900
categories: [dev]
---

# RNN (Recurrent Neural Network) 이해하기

## 1. RNN이란?

RNN(Recurrent Neural Network)은 **순서를 가진 데이터(Sequence Data)**를 처리하는 데 특화된 신경망 구조입니다.  
대표적으로 자연어 처리, 음성 인식, 주가 예측 등 **시계열 데이터**를 다룰 때 사용됩니다.

---

## 2. Sequence Data란?
Sequence Data는 **시간 또는 순서에 따라 변화하는 데이터**를 의미합니다. 예를 들어:
- **텍스트**: 단어 순서에 따라 문장의 의미가 결정됨.
- **음성 신호**: 시간에 따라 변화하는 파형 데이터.
- **주가 데이터**: 시간 순으로 기록된 가격 변화.

---

## 3. RNN의 기본 구조

RNN은 다음과 같은 구조로 이루어져 있습니다:
1. 입력 데이터와 은닉 상태를 처리하여 다음 은닉 상태를 계산.
2. 시간 단계(Time Step) 간 **파라미터를 공유**하여 효율적인 학습 가능.

### 주요 구성 요소:
- **입력 벡터**: 시퀀스의 각 데이터 포인트.
- **은닉 상태**: 이전 단계의 정보를 저장하는 역할.
- **출력**: 시퀀스 데이터의 예측 결과.


---

## 4. RNN의 주요 특징

### 1) 순환 구조
RNN은 **이전 단계의 은닉 상태(hidden state)**를 현재 입력과 함께 처리하여 다음 상태를 계산합니다.  
이를 통해 데이터의 시간적 의존성을 학습합니다.

### 2) 파라미터 공유
RNN은 모든 **시간 단계(Time Step)** 간에 동일한 파라미터를 사용하여 계산을 수행합니다.  
이를 통해 복잡도를 낮추고 학습 효율성을 높일 수 있습니다.

---

## 5. RNN의 다양한 Sequence 모델
RNN은 다양한 입력-출력 구조를 지원합니다:

- **One-to-One**: 일반적인 Fully Connected Neural Network (e.g., 이미지 분류)
- **One-to-Many**: 입력 하나로 여러 개의 출력 (e.g., 이미지 캡션 생성)
- **Many-to-One**: 여러 입력으로 하나의 출력 (e.g., 감성 분석)
- **Many-to-Many**: 여러 입력으로 여러 출력 (e.g., 기계 번역)

---

## 6. RNN과 다른 신경망의 차이

| 모델 | 처리 데이터 | 특징 |
|------|-------------|------|
| FCN  | 고정 크기 데이터 | 순서 정보 없음 |
| CNN  | 공간적 데이터 | 공간 내의 패턴 학습 |
| RNN  | 순서 있는 데이터 | 시간적 패턴 학습 |

---

## 7. RNN의 파생 모델
1. **LSTM(Long Short-Term Memory)**: 장기 의존성 문제를 해결하기 위해 고안된 모델.
2. **GRU(Gated Recurrent Unit)**: LSTM의 경량화 버전.
3. **Attention**: 특정 시점에서 중요한 정보에 집중하도록 도와줌.
4. **Transformer**: Attention 메커니즘을 기반으로 한 비순환 구조.
5. **BERT**: Transformer를 활용한 사전 학습 모델.

---

## 8. RNN의 학습 과정

### Forward Propagation
1. 입력 데이터와 은닉 상태를 결합.
2. 선형 및 비선형 변환을 적용.
3. 출력 계산.

### Backward Propagation
- 출력에서 발생한 오차를 네트워크에 역으로 전파하며 가중치를 업데이트.

---

## 9. RNN의 한계

### 1) Gradient Vanishing & Exploding
- Time Step이 길어지면 **Gradient Vanishing**(기울기 소실) 또는 **Gradient Exploding**(기울기 폭발) 문제가 발생할 수 있습니다.

### 2) 장기 의존성 문제
- 초기 시점의 정보가 이후 시점에 제대로 전달되지 않아, **장기 의존성(Long-term dependency)**을 학습하기 어렵습니다.

---

## 10. 시각 자료

### 1) Sequence Data의 예

### 2) RNN의 Forward Propagation

### 3) RNN의 한계와 LSTM
![RNN vs LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)

---

## 11. 결론
RNN은 순서가 중요한 데이터를 처리하는 데 매우 유용한 모델입니다. 하지만 기울기 문제와 장기 의존성 문제를 해결하기 위해 LSTM, GRU, Attention과 같은 모델들이 발전해 왔습니다.  
현재는 **Transformer**와 같은 모델이 RNN을 대체하며 널리 사용되고 있습니다.
