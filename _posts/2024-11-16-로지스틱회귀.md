---
layout: post
title: "Logistic Regression"
date: 2024-11-16 12:50:00 +0900
categories: [dev]
mathjax: true
---

# 로지스틱 회귀분석(Logistic Regression)

| Feature (독립 변수) | Target (종속 변수) | 분석 방법         |
|----------------------|--------------------|-------------------|
| 계량                | 계량              | 회귀 분석         |
| 계량                | 명목              | 로지스틱 회귀분석 |
| 명목                | 명목              | 로그선형모델      |


명목이 합격 불합격(0 or 1) -> 이진 로지스틱 회귀  
3개 이상 -> 다항 로지스틱  
순서가 있는 경우 -> 순서 로지스틱 회귀  

## Bernoulli 분포
Class 가 2개(성공, 실패)인 Classification 문제를 나타낼 수 있는 확률 분포  
결과를 0, 1사잇값(**확률개념**)으로 변경   

$$
P(Y=y)=
\begin{cases}
p, & \ if \ y=1  \\\\
1-p, & \ if \ y=0
\end{cases}
$$

위 수식을 하나로 표현  <br>

$$P(Y=y)=p^{y}(1-p)^{1-y}, y=0, 1$$<br>

이진분류에서는 로짓함수(sigmoid 함수)를 사용. (딥러닝에서의 Acvation Function중 하나)  

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

$$
\sigma'(x) = \sigma(x) \cdot (1 - \sigma(x))
$$

## odds
무언가 일어나는 비율  
1번 이기고 4번 지는 경우 - 1/4 = 0.25  
지는 경우 : 1/8=0.125, 1/16=0.062 ...0에 수렴  
이기는 경우 : 4/3=1.3, 8/3=2.7.....무한대에 수렴  

1/6 = 0.17, 6/1=6  
로그를 취하면 log(1/6) = -1.79, log(6) =1.79  
절대값이 같아짐.  

$$
odds(p) = \frac{p}{1-p}
$$

$$
logit_function = \log_e\frac{p}{1-p}
$$

## neural network 에서 sigmoid function
- 아무리 큰 값이 들어와도 0,1 사이의 값만 반환  
- 출력값이 모두 양수이기 때문에 출력 가중치 합이 입력 가중치 합보다 커짐.  
- 미분시 아주 간단한 형태가 나옴.  

### backpropagation 시 vanishing gradient 발생
- ReLU Function으로 대체
- 출력값이 모두 양수이기 때문에 linear gradient에서 기울기가 모두 양수거나 음수가 됨.
- 기울기 업데이트가 지그재그로 변동
- 학습 효율성 감소
- hidden layer에서 선형함수와 시그모이드는 안쓰는게 좋음

sigmoid function은 **이진 분류**를 하고자 하는 경우 출력층에서만 사용하는 것을 권고.

## Classification에서의 성능평가
Confusion Matrix

| 실제\예측 | Positive (1) | Negative (0) |
|-----------|--------------|--------------|
| Positive (1) | True Positive (TP) | False Negative (FN) |
| Negative (0) | False Positive (FP) | True Negative (TN) |

## Multinomial Classificaton Problem
다중 분류 문제를 이진 분류 방법(sigmoid function)을 이용하면 각각의 확률을 다 더했을 때 1이 안됨.  

$$
\begin{bmatrix}
2.0 \\
1.0 \\
0.1
\end{bmatrix}
$$

Softmax Function
- Sigmoid 함수를 대체
- 모든 가설값을 0~1로 제한
- 전체 클래스에 대한 합이 1 : 확률 정규화

$$
S_{y_i} = \frac{e^{y_i}}{\sum_{i} e^{y_i}}
$$

**예측값**

$$
\begin{bmatrix}
0.66 \\
0.24 \\
0.10
\end{bmatrix}
$$

$$
D(S, L) = -\sum_{i} L(i) \log(s_i)
$$

**정답**

$$
\begin{bmatrix}
1.0 \\
0.0 \\
0.0
\end{bmatrix}
$$

## Multinomial Classificaton Problem의 Confusion matrix

| 실제\예측 | Class 1       | Class 2       | Class 3       | Class 4       |
|-----------|---------------|---------------|---------------|---------------|
| Class 1   | True Positive (TP1) | False Positive (FP12) | False Positive (FP13) | False Positive (FP14) |
| Class 2   | False Positive (FP21) | True Positive (TP2) | False Positive (FP23) | False Positive (FP24) |
| Class 3   | False Positive (FP31) | False Positive (FP32) | True Positive (TP3) | False Positive (FP34) |
| Class 4   | False Positive (FP41) | False Positive (FP42) | False Positive (FP43) | True Positive (TP4) |

- TP1,TP2,TP3,TP4: 각 클래스에서 예측이 정확히 맞은 경우.
- FPij: 실제 값이 Class i인데, Class j로 잘못 예측된 경우.
